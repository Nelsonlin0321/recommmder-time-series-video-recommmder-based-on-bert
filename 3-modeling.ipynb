{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertPreTrainedModel,AlbertConfig,AlbertModel\n",
    "from torch import nn\n",
    "from typing import Optional,Union,Tuple\n",
    "import torch\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from torch.nn import MSELoss,CrossEntropyLoss,BCEWithLogitsLoss\n",
    "\n",
    "from transformers import AutoConfig\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"albert-base-v2\"\n",
    "category_value_map_dict = utils.open_object(\"./artifacts/col_value_to_index_dict.pkl\")\n",
    "catergory_features = list(category_value_map_dict)\n",
    "\n",
    "numeric_scaler = utils.open_object(\"artifacts/numeric_scaler.pkl\")\n",
    "numeric_features = list(numeric_scaler.feature_names_in_)\n",
    "\n",
    "df_series = utils.open_object(\"./artifacts/series_table.pkl\")\n",
    "series_features = list(df_series.columns)\n",
    "series_features.remove(\"sri_des\")\n",
    "model_config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "model_config.num_lables = 2\n",
    "model_config.add_pooling_layer = False\n",
    "model_config.embedding_size = 4\n",
    "\n",
    "model_config.category_value_map_dict = category_value_map_dict\n",
    "model_config.series_embedding_size = 16\n",
    "model_config.target_feature = 'product_series_cms_id' \n",
    "model_config.catergory_features = catergory_features\n",
    "model_config.numeric_features = numeric_features\n",
    "model_config.series_features = series_features\n",
    "# model_config.bert_output_size = 64\n",
    "# model_config.hidden_sizes = [201+32,128,32]\n",
    "model_config.dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_sizes, dropout=0.1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential()\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            self.mlp.add_module(f'mlp-layers-{i}-Linear', nn.Linear(\n",
    "                in_features=hidden_sizes[i], out_features=hidden_sizes[i+1], bias=True))\n",
    "            self.mlp.add_module(f'mlp-layers-{i}-LeakyReLU', nn.LeakyReLU())\n",
    "            self.mlp.add_module(\n",
    "                f'mlp-layers-{i}-Dropout', nn.Dropout(p=dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "class VedioRecommender(AlbertPreTrainedModel):\n",
    "    def __init__(self, config: AlbertConfig):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = model_config.num_labels\n",
    "        self.model_config = model_config \n",
    "        self.config = config\n",
    "\n",
    "        # bert\n",
    "        self.albert = AlbertModel(config)\n",
    "        self.dropout = nn.Dropout(config.classifier_dropout_prob)\n",
    "        # self.classifier = nn.Linear(config.hidden_size, self.model_config.num_labels)\n",
    "        # Initialize weights and apply final processing\n",
    "        \n",
    "        self.post_init()\n",
    "    \n",
    "        # category embedding\n",
    "        self.feature_embedding_dict = nn.ModuleDict()\n",
    "        for feature in self.model_config.catergory_features:\n",
    "            if feature in self.model_config.series_features:\n",
    "                category_embeddings = nn.Embedding(len(\n",
    "                    self.model_config.category_value_map_dict[feature]), self.model_config.series_embedding_size)\n",
    "            else:\n",
    "                category_embeddings = nn.Embedding(len(\n",
    "                    self.model_config.category_value_map_dict[feature]), self.model_config.embedding_size)\n",
    "\n",
    "            category_embeddings.weight.data.uniform_(-0.1, -0.1)\n",
    "            self.feature_embedding_dict[feature] = category_embeddings\n",
    "\n",
    "        # series\n",
    "        self.series_feature_size = len(self.model_config.series_features)*self.model_config.series_embedding_size*2\n",
    "        # self.series_linear = nn.Linear(self.series_feature_size, 1, bias=True)\n",
    "\n",
    "        # category\n",
    "        self.category_feature_size = len(\n",
    "            [f for f in self.model_config.catergory_features if f not in self.model_config.series_features]) * self.model_config.embedding_size\n",
    "        # self.category_linear = nn.Linear(self.category_feature_size*self.model_config.series_embedding_size, 1, bias=True)\n",
    "\n",
    "        # numeric\n",
    "        self.numeric_feature_size = len(self.model_config.numeric_features)\n",
    "        # self.numeric_linear = nn.Linear(self.numeric_feature_size, 1, bias=True)\n",
    "        \n",
    "        \n",
    "        # # mlp\n",
    "        # self.mlp = MLP(self.model_config.hidden_sizes,dropout=self.model_config.dropout)\n",
    "        \n",
    "        #\n",
    "        self.all_feature_size = config.hidden_size + self.series_feature_size  + self.category_feature_size  + self.numeric_feature_size\n",
    "        \n",
    "        # self.classifier = nn.Linear(config.hidden_size, self.model_config.num_labels)\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=self.all_feature_size,\n",
    "                                out_features=self.model_config.num_labels, bias=True)\n",
    "\n",
    "    def mean_pool_concat_embedding(self,embeddings_value):\n",
    "        embeddings_hist_value = embeddings_value[:,:-1,:]\n",
    "        embeddings_next_value = embeddings_hist_value[:,-1,:]\n",
    "        embeddings_hist_mean_value = torch.mean(embeddings_hist_value,dim = 1)\n",
    "        embeddings_output = torch.concat(\n",
    "            (embeddings_hist_mean_value,embeddings_next_value),dim=1) \n",
    "\n",
    "        return embeddings_output\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        inputs:Optional[dict]=None,\n",
    "    ) -> Union[SequenceClassifierOutput, Tuple]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.albert(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            token_type_ids=inputs[\"token_type_ids\"],\n",
    "        )\n",
    "        \n",
    "        bert_encode = outputs[1]\n",
    "        bert_encode = self.dropout(bert_encode)\n",
    "        \n",
    "        labels = inputs['labels']\n",
    "\n",
    "        # category embedding\n",
    "        series_embedding_tensors_list = []\n",
    "        for feature in self.model_config.series_features:\n",
    "            embedding_ids = inputs[feature]\n",
    "            embedding_tensors = self.feature_embedding_dict[feature](\n",
    "                embedding_ids)\n",
    "            embedding_tensors = self.mean_pool_concat_embedding(\n",
    "                embedding_tensors)\n",
    "            series_embedding_tensors_list.append(embedding_tensors)\n",
    "\n",
    "        series_embedding_encode = torch.concat(\n",
    "            series_embedding_tensors_list, dim=1)\n",
    "\n",
    "        \n",
    "        # category embedding\n",
    "        category_embedding_tensors_list = []\n",
    "        for feature in self.model_config.catergory_features:\n",
    "            if feature not in self.model_config.series_features:\n",
    "                embedding_ids = inputs[feature]\n",
    "                embedding_tensors = self.feature_embedding_dict[feature](\n",
    "                    embedding_ids)\n",
    "                \n",
    "                embedding_tensors = torch.mean(embedding_tensors, dim=1)\n",
    "                category_embedding_tensors_list.append(embedding_tensors)\n",
    "\n",
    "        category_embedding_encode = torch.concat(category_embedding_tensors_list, dim=1)\n",
    "\n",
    "        # numeric\n",
    "        numeric_tensors_list = []\n",
    "        for feature in self.model_config.numeric_features:\n",
    "            tensors = inputs[feature].view(-1, 1)\n",
    "            numeric_tensors_list.append(tensors)\n",
    "\n",
    "        numeric_encode = torch.concat(numeric_tensors_list, dim=1)\n",
    "\n",
    "        all_output = torch.concat([bert_encode, series_embedding_encode,category_embedding_encode,numeric_encode],dim=1)\n",
    "        \n",
    "        logits = self.classifier(all_output)\n",
    "\n",
    "        # pooled_output = outputs[1]\n",
    "\n",
    "        # pooled_output = self.dropout(pooled_output)\n",
    "        # logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2219ec9651574f7391814c16cb90416a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/45.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing VedioRecommender: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight']\n",
      "- This IS expected if you are initializing VedioRecommender from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VedioRecommender from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VedioRecommender were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['feature_embedding_dict.subscription_source.weight', 'feature_embedding_dict.cp_name.weight', 'feature_embedding_dict.product_lang_name.weight', 'feature_embedding_dict.device_network_mode.weight', 'feature_embedding_dict.plan_platform.weight', 'feature_embedding_dict.subtitle.weight', 'classifier.weight', 'feature_embedding_dict.product_series_cms_id.weight', 'feature_embedding_dict.screen_mode.weight', 'feature_embedding_dict.product_cat_name.weight', 'classifier.bias', 'feature_embedding_dict.resolution.weight', 'feature_embedding_dict.platform_name.weight', 'feature_embedding_dict.video_streaming_mode.weight', 'feature_embedding_dict.user_type.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = VedioRecommender.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b007d2981b7fc6aa14922b794f9b4f023f5cfd24ddc48922ef6cc62b5714e3d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
